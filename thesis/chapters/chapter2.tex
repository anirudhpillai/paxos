\chapter{Background}
This chapter lays down all the previous research which the project builds on. Before
going over the design decisions on the project we first need to understand this
background information and look at related work to see different approaches used
to solve the problem.

\section{Distributed Systems}
A distributed system is a model in which processes running on running different
computers, which are connected together in a network, exchange messages to coordinate
their action, often resulting in the user thinking of the entire system as one single
unified computer. A computer in the distributed system is also alternatively referred to as a
processor or a node in the system. Each node in a distributed systems has its
own memory.

We will now go over a few concepts of distributed systems which will help us understand
the characteristics of the protocols that run on these systems. This will lay down the
groundwork for us to understand the Paxos protocol on which this project is based.

\subsection{Asynchronous Environment}
An asynchronous distributed system is one where there are no guarantees about the
timing and order in which events occur.
The clocks of each of the process in the system can be out of sync and may not be
accurate. Therefore, there can be no guarantees about the order in which events occur.
Further, messages sent by one process to another can be delayed for an arbitrary period of time.

A protocol running in an asynchronous environment has to account for these conditions
in its design and try to achieve its goal without the guarantees of timed events.
An asynchronous environment is very common for a real world distributed system but
it also makes reasoning about the system harder because of the aforementioned properties.

\subsection{Fault Tolerance}
A fault tolerant distributed system is one which can continue to function correctly
despite the failure of some of its components. A `failure' of a node or `fault' in a node
means any unexpected behaviour from that node, for example, not responding to messages
or sending corrupted messages.

Fault tolerance is one of the main reasons for using a distributed system as it
increases the chances of your application continuing to functioning correctly and
makes it more dependable. As Netflix mention on their blog
`Fault Tolerance is a Requirement, Not a Feature' \cite{20}.
%% Reference https://medium.com/netflix-techblog/fault-tolerance-in-a-high-volume-distributed-system-91ab4faae74a
With their Netflix API receiving more than 1 billion requests a day, they expect
that it is guaranteed that some of the nodes in their distributed system will fail.
Using a fault tolerant distributed system they are able to ensure that a small failure
in some nodes doesn't hinder the performance of the overall system, hence,
enabling them to achieve their uptime metrics.

Fault tolerant distributed system protocols are protocols which achieve their
goals despite the failure of some of the nodes of the distributed system they run on.
The protocol accounts for the failures and generally specifies the maximum number of
failures and the types of failures it can handle before it stops functioning correctly.

\subsection{State Machine Replication}
%% Reference https://www.cs.cornell.edu/fbs/publications/SMSurvey.pdf

For a client server model, the easiest way to implement it is to use one single server
which handles all the client request. Obviously this isn't the most robust solution
as if the single server fails, so does your service. To overcome the problem you
use a collection of servers each of which is a replica of the original single server and
ensure that each of these `replicas' fails independently, without effecting the other replicas.
This adds more fault tolerance.

State machine replication is method for creating a fault tolerant distributed system
by replicating servers and using protocols to coordinate the interactions of these
replicated servers with the client. Schneider \cite{1} points out how to use
state machine replication to implement fault tolerant services.

A state machine $M$ can be defined as $M = \langle q_0, Q, I, O, \delta, \gamma \rangle$ where \\
\begin{itemize}
  \item $q_0$ is the starting state
  \item $Q$ is the set of all possible states.
  \item $I$ is set of all valid inputs
  \item $O$ is the set of all valid outputs
  \item $\delta$ is the state transition function, $\delta : I \times Q \rightarrow Q$
  \item $\gamma$ is the output function, $\gamma : I \times Q \rightarrow O$
\end{itemize}

The state machine begins in the start state and transitions to other states and
produces outputs when it receives the inputs. The transition and output are found
using the transition and output functions. A deterministic state machine is one
whose state transition and output functions are injective, i.e. multiple
copies of the machine when given the same input, pass through the same order of states
and produce the same output in the same order.

The method of modelling a distributed system protocol as state transition system
was established by Lamport \cite{10} and is very common. It is a critical component
of this project as we will see soon when we need to encode our protocol in Disel.

State machine replication involves modelling our single server, from the client
server model, and using multiple copies (replicas) of the same deterministic
state machine and providing all of them with the input from the client.
As long as one of the replicas does not crash, while resolving the request,
we can successfully return a response to the client.

% \subsection{Byzantine Fault Tolerance}
% Byzantine fault tolerance is the most general form of fault tolerance in which
% any arbitrary form of failure should be defended against. This failure might not
% just be a node crashing or but could also be some node producing inconsistent
% output or having corrupted state. A node with a Byzantine fault might also behave
% `maliciously', for example, by sending different responses for the same question to
% different nodes, or it might present itself as failed to some nodes and as functioning
% to others. It might do this to prevent the distributed algorithm from functioning
% correctly.
%
% The term comes from the `Byzantine Generals Problem' by Lamport et al \cite{3}
% in which a set of nodes must decide on a course of action, but at the same time
% some of the nodes are `malfunctioning' and give `conflicting information to
% different parts of the system'.

\subsection{Consensus Protocols}
For handling faults in your distributed system you need to have replication.
This leads to the problem of making all these replicas agree with each other
to keep them consistent. Consensus protocols try to solve this problem.

\begin{quote}
Consensus protocols are the family of distributed systems protocols which aim to
make a distributed network of processes agree on one result.
\end{quote}

These protocols are of interest because of their numerous real world applications.
Let us take the example of a distributed database, which is a critical part of almost
all large scale real world applications. This distributed database will run
over a network of computers and every time you use the database you aren't guaranteed
to be served by the same computer.

Suppose you add a file to the database. This action is performed by a node, in
the distributed database, that was handling your `add' request.
Later when you want to retrieve the file from the database
you might be served by a different node in the distributed database
that did not perform the `add' request. In order
for the new node to know that the file exists in the system, you will need to use a
consensus protocol which helps all the nodes in the system (which handle user
requests) to agree upon the result that the file has been added to the system.

A popular consensus protocol is the blockchain consensus protocol which powers Bitcoin.
P\^irlea et al \cite{5} have verified a subset of this protocol in Coq. Other
examples of consensus protocols are Raft \cite{12}, Stellar Consensus Protocol \cite{13}
and Paxos \cite{14}, which we look at next.


\section{Paxos}
Having understood the the main concepts behind distributed system protocols, we can
now finally get to the protocol at the heart of this project. Paxos is a family of
asynchronous, fault tolerant, consensus protocol which achieves consensus in a network
of unreliable processes as long as a majority of them don't fail. Paxos was
outlined in Lamport's 1998 paper, `Part Time Parliament' \cite{4}.

Paxos is used for state machine replication. Once you have multiple replicas
servicing client requests, how do you makes sure that all of these replicas agree
on what action to take? The solution is simply to use a consensus protocol like
Paxos to make all replicas agree on something. Paxos is heavily used in building
software. It has been used at Google to build a fault tolerate database \cite{11}
and the Chubby \cite{12} lock service.

Paxos has many variants but the one we will focus on is the one we actually prove
in Disel, single decree Paxos, also know as simple Paxos. Simple Paxos is an algorithm
that helps a distributed network of processors to achieve consensus.
Consensus is achieved when the network of processor agree on a common value.

For simple Paxos, we assume the following assumptions hold about the processors
and the environment, in order for the protocol to function correctly.
\begin{itemize}
  \item Processors communicate between each other by exchanging asynchronous messages between them.
  \item Processors run at an arbitrary speed and may fail or restart. Handling this relates
    to the fault tolerant nature of Paxos. Also, we assume that Byzantine faults don't occur.
    This means that all processors actually work together to try to achieve consensus on a value.
    There are variants of Paxos which can also handle Byzantine failure buy not simple Paxos.
    (This can be linked to the `Practical Byzantine Fault Tolerance' paper, by Castro et al \cite{2},
    which states that any algorithm handling
    Byzantine faults must have three phases. Simple Paxos only has two phases.)
\end{itemize}

As for fault tolerance of Paxos, in order to handle a failure of up to $f$ processors,
we need to have a minimum $2f + 1$ processors participating in the algorithm. This
means Paxos functions correctly as long as a majority of the processors in the
network do not fail. We will see shortly why just a majority needs to be functioning
correctly.

A processor participating in simple Paxos, may have one or more of these three
different roles - proposer, acceptor or learner.
\begin{itemize}
  \item \textbf{Proposer} - A process acting as a proposer listens for client
    request and proposes a value which the network of processes tries to agree upon.
  \item \textbf{Acceptor} - acceptors receive proposed values from the proposers
    and then respond to them stating whether they are in a position to accept the value or not.
    For a proposed value to be accepted, a majority of all the existing acceptors
    have to accept the proposed value.
  \item \textbf{Learner} - The learner has to be informed when an acceptor accepts a value.
    The learner can then figure out when consensus has been achieved by monitoring
    when a majority of acceptors have accepted the same proposal.
    Once the acceptors agree on a value, the learner may act on the value by,
    for example, sending a request to the client informing them about the agreed value.
\end{itemize}

\vspace{-4mm}
\subsection{Choosing a Value}
For passing around the value to be chosen from one processor to the other,
a processor must send a 'proposal' to the other processor.
You can think of a proposal as just a tuple $\langle n, v \rangle$.
$n$ is just a natural number associated with a proposal which makes
it easy to keep track of all the different proposals and $v$ is the value
the value that is being proposed.

A \texttt{quorum} of acceptors is a subset of the set of all acceptors
and has a length greater
than $N / 2$ where $N$ is the length of the set of acceptors. A \texttt{quorum} is just
a set denoting a majority of all the available acceptors.

\begin{quote}
Consensus is achieved when a proposal is accepted by a majority of acceptors.
\end{quote}

\vspace{-4mm}
\subsubsection{The Algorithm}
Simple Paxos runs in rounds until consensus is achieved (a successful round
occurs when a majority of acceptors accept a proposal).
A successful round of the algorithm has two phases, each of which can
be subdivided into parts a, b.

\begin{itemize}
  \item \textbf{Phase 1a: Prepare Request.}
    A proposer sends a proposal
    $\langle n, v \rangle$ to each acceptor in any randomly chosen \texttt{quorum} of acceptors.
    This first message that the proposer sends out is called a \texttt{prepare request}.
    This phase can be thought of as the proposer trying to
    `prepare' the acceptors to `accept' a value in the future.
  \item \textbf{Phase 1b: Promise Response.}
    An acceptor on receiving a prepare request, responds with a \texttt{promise response},
    if and only if the acceptor has not already sent a promise response with
    a proposal containing a proposal number $n'$ where $n' > n$.

    A promise response for proposal $\langle n, v \rangle$ is basically a
    guarantee (a 'promise') that this acceptor will not respond to any
    messages with proposals that have a proposal number $n'$ where $n' \leq n$.

    Thus, if an incoming prepare request has proposal number that is not
    greater than what the
    acceptor has already promised earlier, then the acceptor can ignore this
    prepare request by not responding to it. Although, for speeding up the
    protocol, the acceptor can send out a \texttt{nack response} which tells the
    proposer to stop trying to achieve consensus with this proposal.

    If the acceptor has not already accepted a proposal, then the body of
    the promise response can be empty, otherwise, the acceptor must include in it the
    last proposal that it accepted.
  \item \textbf{Phase 2b: Accept Request.}
    If the proposer successfully receives promise responses from a majority of
    acceptors, then it can send out an \texttt{accept request}. A accept request is
    a message containing a proposal which tells an acceptor to accept this
    proposal if it can.

    The proposer creates a new proposal, $\langle n, v' \rangle$ where $n$ is
    the same as in the proposal which the proposer sent in its prepare request.
    But, $v'$ is the value from the highest numbered proposal, selected from all
    the proposals that the proposer receives in the promise responses.
    If none of the promise responses received by the proposer contain a proposal,
    the proposer is free to set $v'$ to any value it likes.
    The proposer then sends this accept request with proposal $\langle n, v' \rangle$
    to another \texttt{quorum} of acceptors.
  \item \textbf{Phase 2b: Accepted Response.}
    Any acceptor that receives the accept request with proposal $\langle n, v \rangle$,
    responds with an \texttt{accepted response} if and only if it hasn't already promised
    not to respond to any proposals with proposal number $n'$ where $n' >= n$.
    The \texttt{accepted response} is sent to the learner and contains the proposal
    that was accepted by the acceptor.
\end{itemize}

\begin{figure}
\centering
\includegraphics[width=\textwidth]{figures/paxos_run.png}
\caption{A run of Paxos
\label{fig:myInlineFigure}}
\end{figure}

\vspace{-4mm}
\subsection{Informing learner}
When consensus is achieved, a learner must be informed that a majority of acceptors
have agreed on a value. There are various ways to do this.

\begin{enumerate}
  \item Whenever an acceptor accepts a value, it should send the accepted proposal
    to all the learners (\texttt{accepted response}).
    The learners will then know when a majority of acceptors
    have accepted the same value.
  \item We can have a distinguished learner which informs other learners about
    the choose value. The acceptors only need to inform this particular learner
    when they accept a value. This reduces number of messages sent but the
    distinguished learner becomes the single point of failure and also requires
    an additional round of sending messages where the distinguished learner informs
    other learners that a value has been chosen.
  \item We can use a set of distinguished learners. The acceptors inform these
    distinguished learners who then inform the other learners. This increases
    reliability but also increases the number of messages exchanged.
\end{enumerate}

\vspace{-5mm}
\section{Disel}
Having learnt the concepts behind distributed systems and the Paxos protocol, we
can now look at Disel.
Disel \cite{9} is a verification framework, built on top of the Coq theorem prover,
that enables one to prove the safety properties of a distributed protocol by
breaking down the protocol into its state space invariants and its atomic properties.
The unique aspect of Disel is that it also allows one to combine these separate
verified protocols to create a complete verified system. Additionally, the
code extraction properties of Disel enables one to extract the verified runnable
OCaml code for the protocols which can be combined with an shim that supplies
the implementation of the various primitive operations like sending or receiving
a message.

\subsection{Protocol Encoding}
We will now look at how Disel requires the state space of a protocol to be encoded in it.
The figure \ref{fig:DiselStateSpace} shows the distributed state space and the world
components in Disel.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{figures/disel_state_space.jpeg}
\caption{Disel's distributed state space and the world components as shown in \cite{9}
\label{fig:DiselStateSpace}}
\end{figure}

As you can see from \ref{fig:DiselStateSpace}, a \textsf{statelet} is a component
in the protocol and consists of the \textsf{MessageSoup}
and the \textsf{DistLocState}. A message soup is `finite partial map from unique message
identifiers to messages,
each of which carries its sender and recipient node ids, the payload $m$, which
includes a tag, and a boolean indicating whether the message is already received
or not yet' \cite{9}. While the local state of a node
`maps each node id into protocol-specific piece of local state, represented as a
mapping from locations (isomorphic to natural numbers) to specific values' \cite{9}.

Additionally, a protocol $P$ in Disel is defined as a tuple
of the \textsf{Coherence}, the set of send transitions and the set of receive transitions.
Let us now look at each of these components in detail.

A \textsf{Coherence} is predicate, i.e., a function that takes in a statelet and returns a proposition
indicating whether the statelet is valid or not. Thus, the coherence allows us
to impose constraints on the local state of each node and on the message soup.

A transition is defined as a tuple of consisting of the following:
\begin{enumerate}
  \item Tag - a unique natural number identifier for the message to be sent in the transition.
  \item Precondition - The constraints that are imposed on identity of the sender of the message,
    identity of the receiver is, the message that is being sent and on the local state of
    the sender/receiver (depending on whether it is a send transition/receive transition).
  \item Step function - Describes how the local state of the sender/receiver changes after
    making the transition.
\end{enumerate}

You can see in the code example below how the step function and pre condition
are encoded for sending the \texttt{prepare request} in Paxos. \texttt{PInit},
\texttt{PSentPrep} and \texttt{PWaitPrepResp} are some states the
proposer can be in. The precodition requires the state of proposer \texttt{p}
to be either \texttt{PInit} or \texttt{PSentPrep} and holding things required
in the specific state.

\begin{lstlisting}
(* Changes in the Node state triggered upon send *)
Definition step_send (s: StateT) (to : nid) (p: proposal): StateT :=
    let: (e, rs) := s in
    match rs with
    ...
    (* Step function for the sending prepare request *)
    | PInit p' =>
      if acceptors == [:: to] (* if only one acceptor *)
      then (e, PWaitPrepResp [::] p')
      else (e, PSentPrep [:: to] p')
    ...
    | _ => (e, rs)
    end.

(* Precondition for send prepare request transition *)
Definition send_prepare_req_prec (p: StateT) (m: payload) :=
  (exists n psal, p = (n, PInit psal)) \/
  (exists n tos psal, p = (n, PSentPrep tos psal)).
\end{lstlisting}

\subsection{Protocol to Programs}
The state transitions that we implement in the protocol encoding phase, are the
first step towards creating executable programs using Disel.
We can then use the library of \textit{transition wrappers} provided by Disel
that allow one to decorate low level send/receive primitives with the transitions
that we have defined. These decorated primitives can later on be used to extract
code for executable programs. In Chapter 5, we will dive into the details of
how we extracted the code for our client application running Paxos.

The \texttt{send\_action\_wrapper} wrapper provided by Disel takes a send transition encoded
by us and returns a program that will send a message.
\begin{lstlisting}
Program Definition send_prepare_req psal to :=
  act (@send_action_wrapper W paxos p l (prEq paxos)
       (send_prepare_req_trans proposers acceptors) _ psal to).
\end{lstlisting}

The \texttt{tryrecv\_action\_wrapper} is similar but the main difference is that in
a received transition, we may receive messages from any of the multiple protocols
that might be executing at the time. To address this problem, we need to check the
tag $t$ returned by the receive wrapper and ensure that this tag belongs to the
protocol that was specified in the wrapper. In the code example below, we check
that the received message is either a \texttt{promise\_resp} or a \texttt{nack\_resp}
both of which belong to the \texttt{paxos} protocol and are valid responses to a
\texttt{prepare\_req}.

\begin{lstlisting}
(* Non blocking receive *)
Program Definition tryrecv_prepare_resp := act (@tryrecv_action_wrapper W p
      (* filter *)
      (fun k _ t b => (k == l) && ((t == promise_resp) || (t == nack_resp))) _).
\end{lstlisting}

If an incoming message matches the conditions specified, the wrapper returns
\texttt{Some(from, m)} where $m$ is the message and $from$ is the sender. Otherwise
it returns \texttt{None}.

These low level primitives can then be combined together for specifying the roles
of each node in the protocol. We can use the \texttt{send\_prepare\_req} to
come up with \texttt{send\_prepare\_req\_loop} which every proposer performs when
it starts up. These functions can then further be combined together to give the
entire implementation of a node. \texttt{proposer\_round} below is the program
that each node acting a proposer executes.

\begin{lstlisting}
Program Definition send_prepare_req_loop e (psal: proposal):
  {(pinit: proposal)}, DHT [p, W]
  (fun i => loc i = st :-> (e, PInit pinit),
   fun r m => r = tt /\
              loc m = st :-> (e, PWaitPrepResp [::] pinit)) :=
  Do (ffix (fun (rec : send_prepare_req_loop_spec e) to_send =>
              Do (match to_send with
                  | to :: tos => send_prepare_req psal to ;; rec tos
                  | [::] => ret _ _ tt
                  end)) acceptors).

Program Definition proposer_round (psal: proposal):
  {(e : nat)}, DHT [p, W]
  (fun i => loc i = st :-> (e, PInit psal),
   fun res m => loc m = st :-> (e.+1, PAbort))
  :=
  Do (e <-- read_round;
      send_prepare_req_loop e psal;;
      recv_promises <-- receive_prepare_resp_loop e;
      check <-- check_promises recv_promises;
      if check
      then send_accept_reqs e (choose_highest_numbered_proposal psal recv_promises)
      else send_accept_reqs e [:: 0; 0]).
     (* If check fails then send an acc_req for (0, 0) which will never be
        accepted by any acceptor *)
\end{lstlisting}

Once this implementation has been finished in Disel we can use Disel's extraction
capabilities to extract the OCaml code for executing the program. (Outlined in Chapter 5)

\subsection{Inductive Invariant}
As many other verification tools for distributed protocols, Disel relies on using
inductive invariants to prove the correctness of the protocol. In this section,
we look at what are invariants and inductive invariants.

%% or more intuition, check Chapter 5 of http://adam.chlipala.net/frap/frap_book.pdf
A invariant in a program is a property of a program that always holds true,
from the start through to the end of execution of the program. An invariant can be for something
more specific like a function or even a loop. The only requirement is the property
described by the invariant should always hold before, during and after execution
of the part of that code. To put it more formally,
`An invariant of a transition system is a property that is
always true, in all of the systemâ€™s reachable states.' \cite{21}.

The problem making an assertion like $x > 2$, at a certain point of a program execution,
is that you may assume that the it holds at that point in the program execution, but you still
do not have a guarantee that it will hold before, during and after the
execution of the program, i.e. hold in any state of the program.

Therefore, we need to make our invariants $inductive$. An inductive invariant
of a program, is an invariant which if it holds in a particular state $s$ of the
program, it is guaranteed to hold in all states of the program reachable from $s$.
Thus, having an inductive invariant that holds in the start state of a program
is much more useful, as we can be sure that the invariant will continue to
hold throughout and after the execution of the program.

This means that once you establish an inductive invariant from the given invariants
of a protocol and a starting state for the protocol in which the invariant holds,
in order to show that the protocol maintains the invariant, you only need to
prove the inductive invariant maintains the induction property over any possible state
transition in the protocol. This is exactly how you use an inductive invariant in
Disel. Developing and encoding the inductive invariant for the protocol in Disel
is the way to verify it.


\section{Related Work}
\label{sec:relatedWork}
People have attempted to create software for verfying the correctness of distributed
systems. IronFleet \cite{15}, developed at Microsoft Research, can prove not just
safety but also liveness properties of distributed systems and has been used
to verify a `complex implementation of a Paxos-based replicated state machine
library'. Verdi \cite{16} is another framework built on top of Coq and has been used
to develop the `proof of linearizability of the Raft state machine replication
algorithm'. Ivy \cite{19} is another tool which makes it much easier to find and
prove inductive invariants for the system. Other similar tools are PSync \cite{17}
and EventML \cite{18}.
